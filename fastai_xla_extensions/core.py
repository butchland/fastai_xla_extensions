# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_utils.ipynb (unless otherwise specified).

__all__ = ['XLA_AVAILABLE', 'XLAOptimProxy', 'XLAOptCallback', 'ChromeTrace']

# Cell
#colab
XLA_AVAILABLE = True
try:
    import torch_xla.core.xla_model as xm
except ImportError as e:
    XLA_AVAILABLE = False
    import warnings
    # warnings.warn('fastai_xla_extensions requires Pytorch-XLA, will revert to default',
    #              RuntimeWarning)

# Cell
if not XLA_AVAILABLE:
    from types import SimpleNamespace
    import torch.cuda
    def fake_opt_step(opt,barrier=False):
        opt.step()
    def fake_device(n=None, devkind=None):
        gpu_available = torch.cuda.is_available()
        return torch.device(torch.cuda.current_device()) if gpu_available else torch.device('cpu')
    xm = SimpleNamespace(
        optimizer_step = fake_opt_step,
        xla_device = fake_device
    )


# Cell
if XLA_AVAILABLE:
    from fastcore.foundation import defaults
    defaults.tpu_device = xm.xla_device(devkind='TPU')
    defaults.tpu_available = defaults.tpu_device != None

# Cell
if XLA_AVAILABLE and defaults.tpu_available:
    import fastai2.torch_core
    from fastai2.torch_core import apply
    from torch import Tensor
    def default_device(use_cuda=-1):
        "Return `TPU` as default device"
        return defaults.tpu_device
    def to_device(b, device=None):
        "Recursively put `b` on `device`."
        if device is None: device=default_device()
        # print(f'setting device to {device}')
        def _inner(o): return o.to(device, non_blocking=True) if isinstance(o,Tensor) else o.to_device(device) if hasattr(o, "to_device") else o
        return apply(_inner, b)

    fastai2.torch_core.default_device = default_device
    fastai2.torch_core.to_device = to_device


# Cell
class XLAOptimProxy:
    "Proxy optimizer to override `opt.step` with Pytorch XLA sync method `xm.optimizer_step` "
    def __init__(self,opt):
        self.opt = opt

    def xla_step(self):
        xm.optimizer_step(self.opt,barrier=True) # sync on gradient update

    def __getattr__(self,name):
        if name == 'step': # override proxying for step method
                return getattr(self,'xla_step')
        # proxy everything else
        return getattr(self.opt,name)

# Cell
from fastai2.callback.core import Callback

class XLAOptCallback(Callback):
    def begin_fit(self):
        'replace opt with proxy which calls `xm.optimizer_step` instead of `opt.step`'
        if self.learn.opt is not None:
            if not isinstance(self.learn.opt,XLAOptimProxy):
                opt = self.learn.opt
                self.learn.opt = XLAOptimProxy(opt)

    def after_fit(self):
        'restore original opt '
        if isinstance(self.learn.opt, XLAOptimProxy):
            opt = self.learn.opt.opt
            self.learn.opt = opt


# Cell
if XLA_AVAILABLE and defaults.tpu_available:
    if hasattr(defaults,'callbacks'):
        if XLAOptCallback not in defaults.callbacks:
            defaults.callbacks.append(XLAOptCallback)
    else:
        defaults.callbacks = [XLAOptCallback]

# Cell
import threading
import time
from torch.autograd.profiler import FunctionEvent, EventList
from fastai2.callback.core import Callback

class ChromeTrace(Callback):
    def __init__(self, ll):
        super().__init__()
        self.l = ll
        self.d = {}
    def __call__(self, event_name):
        tid = threading.get_ident()
        tname = threading.current_thread().name
        if event_name.startswith('begin'):
            evt = event_name.replace('begin_', '')
            self.d[evt] = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID)/1000
        elif event_name.startswith('after'):
            evt = event_name.replace('after_', '')
            end = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID)/1000
            if evt in self.d:
                start = self.d[evt]
                fe = FunctionEvent(tname, tid, evt, tid, start, end)
            else:
                # hack if there is only after_xxx, we need a start 500, 1000, 5000?
                start = end-500 # 1-time after_xxx
                fe = FunctionEvent(tname, tid, evt, tid, start, end)
            self.l.append(fe)
            self.d.pop(evt, None)
    def export_chrome_trace(self, fname="export_chrome_trace.json", dumps=False):
        with open(fname, mode="w+") as f:
            prof = EventList(self.l)
            prof.export_chrome_trace(f.name)
            if dumps:
                parsed = json.load(f)
                print(json.dumps(parsed, indent=4, sort_keys=True))